# Comprehensive Linear Regression Analysis and its extention to Boston Housing Dataset

This project delves into the intricacies of linear regression and its extensions through a series of detailed experiments, employing MATLAB for implementation. The project begins with demonstrating the equivalence of two matrix expressions in ridge regression, showcasing deep mathematical insights into regularization techniques. Subsequently, it explores the time efficiency of spectral clustering with varying dimensions, revealing the impact of dimensionality on computational performance.

The core of the project involves a thorough investigation into linear regression models using the Boston housing dataset, which includes a mix of continuous and binary variables. The experiments are meticulously designed to cover a wide range of scenarios:

**1. Data Preprocessing**: We shuffle and split the dataset into training and testing sets to ensure unbiased evaluation, emphasizing the importance of data preprocessing in machine learning.

**2. Model Evaluation**: The project evaluates model performance across different training set sizes, polynomial feature degrees, and regularization strengths. This includes:

* Analyzing mean squared error (MSE) trends to understand model bias and variance.
* Expanding feature sets with polynomial degrees to examine overfitting and model complexity.
* Applying ridge regression to investigate the balance between model accuracy and complexity.
* Utilizing the Lasso method to explore feature selection and regularization effects.
* Comparative Analysis: A comparative analysis of vanilla linear regression, ridge regression, Lasso, and subset selection models is provided, showcasing the trade-offs and benefits of each approach. This section is enriched with mathematical derivations and practical examples, highlighting the nuanced differences between the models.

**3. Implementation Insights**: The project offers insights into MATLAB functions and custom scripts, enhancing understanding of linear algebra applications in machine learning. It also includes critical evaluations of model performance, with discussions on error trends and the implications of model choices.

**4. Results Visualization**: Results are visualized through plots comparing training and testing errors across different model configurations. These visual aids are crucial for interpreting the effects of training set size, polynomial feature complexity, and regularization parameters on model performance.

This project not only demonstrates technical proficiency in implementing and evaluating linear regression models but also showcases an ability to derive and understand complex mathematical relationships within machine learning. Through a blend of theoretical analysis and practical experimentation, this project highlights the versatility and depth of linear regression techniques in addressing real-world data challenges.
